---
title: Towards Modern Data Management
date: 2018-07-21
tags: ['data','modern-data-management']
---

The act of programming becomes more enlightening with every additional environment you work in. My personal journey in data management began with SAS programming. I leaned how to effectively manage analytical pipelines and eventually got certified. Afterwards, I moved on to managing the classic Microsoft stack for a fast moving startup. Many people have known this stack: SQL Server, SSIS, SSRS. It works, but can leave something to be desired. I then went full cloud and learned to tune and optimize data pipelines on AWS - a transition that many companies are currently undergoing.

What have I learned from all this? I’ve learned that any given individual’s technical knowledge can be incomplete in subtle but crucial ways. I’ve also learned that data management has relatively few approaches but relatively many options. Just learning the names of available solutions and their relationships is an exhausting exercise.

## The World Today

But I am more excited about the space of data management than I ever have been. The world is fundamentally different now, and it changes every day. Open-source technologies like [Superset](https://github.com/apache/incubator-superset) provide interesting possibilities for representing relational data in a world that historically has been locked inside of vendor-specific tools and workflows. These tools allow for defining data transformations in reusable code templates that allow for solving problems with first-principles thinking. The ability to read data from anywhere, store it, model it, and present it for analysis has never been easier. And if you are just getting started with business, your [analytics roadmap](https://thinkgrowth.org/the-startup-founders-guide-to-analytics-1d2176f20ac1) has gained a lot of clarity.

Further, Slack channels and open-source Github repositories change the way people can learn about data management tools. Best practices and earned knowledge can be shared across the industry in way that it couldn’t a few years ago.  Because information silos were for the most part vendor-locked, knowledge remained tribal. Moving from Oracle to SQL Server was possible, but less common than it should have been.

This can be contrasted with application development which broke out of most vendor silos a long time ago. In this world, most questions can be answered by the nearest query to Stack Overflow. The community quickly converges on best practices that transcend any one platform or programming language. This is largely driven by the fact that open source encourages knowledge sharing, whereas vendor ecosystems don’t. A rising tide raises all ships.

There are many small changes that companies at all levels are still discovering. Below, I’ve highlighted a few of the more recent disruptions in data management I find interesting.

In predictive modeling, a data management stack might start with Pandas, Jupyter, and scikit-learn. For processing that data, the stack might also add [Dask](https://dask.pydata.org/en/latest/) if it is hip, and Spark if it isn’t. Joking aside, solutions like Dask are exciting, because they provide opportunities for data people to deploy pipelines even faster than with Spark (deep learning tasks excluded), since much of the code developed using Pandas will not need to be rewritten to work with Spark’s APIs. And open source packages like [Prophet](https://facebook.github.io/prophet/) reduce error-prone and duplicative work that has already been solved in a standard and community-approved way. These types of alternatives are not groundbreaking by any means, but they are incremental improvements to a landscape that often times has too much mystique surrounding it.

Secondly, many descriptive analytics stacks are now leveraging templating tools like [dbt](https://github.com/fishtown-analytics/dbt) and orchestration tools like [Airflow](https://github.com/apache/incubator-airflow) that previously could only be found in large enterprise solutions. These tools provide a level of detailed resolution for processing jobs that isn’t feasible with just cron. Additionally, the maturity of the templating environments in these tools provides a reusability and composability that moves towards sustainable infrastructure growth. And the just recently announced [Meltano](https://about.gitlab.com/2018/08/01/hey-data-teams-we-are-working-on-a-tool-just-for-you/) is another confirmation that the tide is changing in favor of open source.

In big data (which I interpret as +1TB), the Apache ecosystem continues to disrupt, with new approaches to problems being experimented with every day. For instance, S3 as a storage solution for infrequently accessed warehouse data is a novel idea that is superior to the previous solution of restoring and querying data from archival tape. Presto is the current market favorite in open source S3 querying technologies, and is an approach to retrieval that I believe has yet to reach its full potential.

## The World Tomorrow

Modern data management has begun moving towards a post-vendor landscape. This doesn’t mean that vendors will be absent, but that they will look different than the previous generation. For instance, Confluent is the commercial company that provides premium support to Kafka, a free and open-source data streaming tool. With the recent additions of KQL and compacted topics for Kafka, the idea of using it in a fashion similar to a traditional database has become slighter closer to reality. An idea like this is fascinating, because of the dynamics it opens up for JIT analysis and JIT decisions.

Some other thoughts: CQRS and event sourcing are wonderful techniques for addressing reactive data concerns popular in web development. In the context of stream processing, frameworks seem to incorporate these ideas already, but the general pattern is still useful even at higher levels of abstraction. Regarding storage, it is cheaper than it ever has been, which opens up the possibility of separate data stores for development, staging, and production. This means that the staging database that has historically been more of a wild-west area of the data stack than an essential part of the data workflow, can now have more structure surrounding it. And adopting a column-store like Redshift for dashboards opens up more flexible modeling possibilities like wide denormalized tables, rather than modeling raw data into a star schema. This can save time and keep pipelines smaller.

However the data stack at a company evolves, the most important thing is to continually think in systems that stream data to each other, rather than silos that must be "integrated". Jay Kreps words it nicely in [The Log](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying):

> So maybe if you squint a bit, you can see the whole of your organization's systems and data flows as a single distributed database. You can view all the individual query-oriented systems (Redis, SOLR, Hive tables, and so on) as just particular indexes on your data. You can view the stream processing systems like Storm or Samza as just a very well-developed trigger and view materialization mechanism.

Traditionally, the domain of data management has been something of a black sheep in the technology departments of some organizations. The stigma is partly due to the tools historically used in data warehousing, and their distinct structural and ideological differences from the tools used to build software. But now that the differences are dissolving, the world of data management will start to look very different at a lot of companies in the coming years. For some, it already does.